{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and DataSet\n",
    "# Dataset is composed of RAMAN TR, RAMAN RE, NIR TR, NIR RE, Compression Force curve, Compression max, and the Dissolution\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pylab\n",
    "import math\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "from numpy import loadtxt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "RamanTR = loadtxt(r'.\\RAMANTR256.csv', delimiter=';', dtype='float64')\n",
    "RamanRe = loadtxt(r'.\\RAMANRE256.csv', delimiter=';', dtype='float64')\n",
    "NIRRe = loadtxt(r'.\\NIRRE256.csv', delimiter=';', dtype='float64')\n",
    "NIRTR= loadtxt(r'.\\NIRTR256.csv', delimiter=';', dtype='float64')\n",
    "Diss = loadtxt(r'.\\DISSOLUTION256.csv', delimiter=';', dtype='float64')\n",
    "Comp = loadtxt(r'.\\Compression256.csv', delimiter=',', dtype='float64')\n",
    "CompMax = loadtxt(r'.\\CompressionMax.csv', delimiter=';', dtype='float64')\n",
    "Contents = loadtxt(r'.\\Contents.csv', delimiter=',', dtype='float64')\n",
    "XramanRe = RamanRe[:,0:1691]\n",
    "XnirRe = NIRRe[:,0:1555]\n",
    "XramanTR = RamanTR[:,0:1691]\n",
    "XnirTR = NIRTR[:,0:713]\n",
    "Xcomp = Comp[:,0:6036]\n",
    "Contents = Contents[:,0:2]\n",
    "XcompMax = np.array(CompMax[np.newaxis].T)\n",
    "Diss=Diss[:,1:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Contents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcf2(P,T,rows,samples):  \n",
    "    rows=rows\n",
    "    samples=samples\n",
    "    count=0\n",
    "\n",
    "    y_pred=P\n",
    "    y_test=T\n",
    "    ftotal=0\n",
    "    for i in range(0, samples):\n",
    "\n",
    "        f=0\n",
    "\n",
    "        somme=0\n",
    "        Xrow = y_test[i]\n",
    "        Yrow = y_pred[i]\n",
    "        for n in range(0,rows):\n",
    "            Xcell=Xrow[n]\n",
    "            Ycell=Yrow[n]\n",
    "            Error=Xcell-Ycell\n",
    "            Eabs=Error**2\n",
    "            somme=somme+Eabs\n",
    "        step1=1+(1/rows)*somme\n",
    "        step2=step1**(-0.5)\n",
    "        step3=step2*100\n",
    "        f=50*math.log10( step3 )\n",
    "        ftotal=ftotal+f\n",
    "    averagef2=ftotal/samples\n",
    "    return(averagef2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 2)\n"
     ]
    }
   ],
   "source": [
    "#Data Preprocessing (Scaling)\n",
    "XramanRescaler = StandardScaler()\n",
    "ScaledRamanRE=XramanRescaler.fit_transform(XramanRe)\n",
    "\n",
    "XnirRescaler = StandardScaler()\n",
    "ScaledNirRE=XnirRescaler.fit_transform(XnirRe)\n",
    "\n",
    "XramanTRscaler = StandardScaler()\n",
    "ScaledRamanTR=XramanTRscaler.fit_transform(XramanTR)\n",
    "\n",
    "XnirTRscaler = StandardScaler()\n",
    "ScaledNirTR=XnirTRscaler.fit_transform(XnirTR)\n",
    "\n",
    "Xcompscaler = StandardScaler()\n",
    "ScaledComp=Xcompscaler.fit_transform(Xcomp)\n",
    "\n",
    "ContentScaler = StandardScaler()\n",
    "ContentsScaled=ContentScaler.fit_transform(Contents)\n",
    "print(ContentsScaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input1 (296, 33)\n",
      "Dissolution (296, 53)\n"
     ]
    }
   ],
   "source": [
    "# Different Inputs and Data Reduction using PCA \n",
    "#INPUT1 ---> PCAINPUT1 \n",
    "#In this input, the data was scaled, merged together, scaled again, PCA was applied to keep 99% of the variances\n",
    "\n",
    "INPUT1=np.hstack((ScaledNirTR,ScaledRamanRE,ScaledComp,ScaledNirRE,ScaledRamanTR))\n",
    "INPUT1scaler = StandardScaler()\n",
    "INPUT1scaler.fit(INPUT1)\n",
    "ScaledINPUT1=INPUT1scaler.transform(INPUT1)\n",
    "pca= PCA(0.99)\n",
    "pca.fit(ScaledINPUT1)\n",
    "PCAINPUT1= pca.transform(ScaledINPUT1)\n",
    "print(\"Input1\",PCAINPUT1.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dissolution\",Diss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Contents (296, 35)\n"
     ]
    }
   ],
   "source": [
    "withContents=np.hstack((PCAINPUT1,Contents))\n",
    "print(\"With Contents\",withContents.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 1163\n",
      "Cross-validated scores: [69.64891502 72.31749615 73.33285967 72.3376714  71.68473255]\n",
      "Average Score: 71.86433495657961\n",
      "Standard Deviation: 1.2268911791145038\n",
      "95% Confidence Interval: 70.7889174222016 to 72.93975249095762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Assuming PCAINPUT1 and Diss are your feature and target variables\n",
    "trainings = 1\n",
    "DissolutionPoints = 53\n",
    "TestSize = 49\n",
    "f = {}\n",
    "average = 0\n",
    "\n",
    "# Assuming PCAINPUT1 and Diss are your feature and target variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(withContents, Diss, test_size=0.165)\n",
    "\n",
    "# Custom scoring function\n",
    "def custom_metric(y_true, y_pred, dissolution_points, test_size):\n",
    "    # Replace the following line with your own custom metric calculation (calcf2)\n",
    "    score = calcf2(y_true, y_pred, dissolution_points, test_size)\n",
    "    return score\n",
    "\n",
    "# Make scorer for the custom metric\n",
    "custom_scorer = make_scorer(custom_metric, greater_is_better=True, dissolution_points=DissolutionPoints, test_size=TestSize)\n",
    "\n",
    "for i in range(0, trainings):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(10, 10, 10), activation='relu', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(mlp, X_train, y_train, cv=5, scoring=custom_scorer)\n",
    "\n",
    "    # Train the model on the entire training set\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    mpl_score = mlp.score(X_test, y_test)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "\n",
    "    # Store performance metric\n",
    "    f[i] = calcf2(y_test, y_pred, DissolutionPoints, TestSize)\n",
    "    average += f[i]\n",
    "\n",
    "\n",
    "    average_score = np.mean(cv_scores)\n",
    "    std_deviation = np.std(cv_scores)\n",
    "    confidence_interval = 1.96 * (std_deviation / np.sqrt(len(cv_scores)))\n",
    "\n",
    "# Get the number of parameters\n",
    "num_params = sum([p.size for p in mlp.coefs_]) + sum([p.size for p in mlp.intercepts_])\n",
    "\n",
    "print(\"Number of parameters in the model:\", num_params)\n",
    "# Print results\n",
    "print(\"Cross-validated scores:\", cv_scores)\n",
    "print(\"Average Score:\", average_score)\n",
    "print(\"Standard Deviation:\", std_deviation)\n",
    "print(\"95% Confidence Interval:\", average_score - confidence_interval, \"to\", average_score + confidence_interval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
